 /***************************************************************************************
 * Copyright (c) 2020-2021 Institute of Computing Technology, Chinese Academy of Sciences
 * Copyright (c) 2020-2021 Peng Cheng Laboratory
 *
 * XiangShan is licensed under Mulan PSL v2.
 * You can use this software according to the terms and conditions of the Mulan PSL v2.
 * You may obtain a copy of Mulan PSL v2 at:
 *          http://license.coscl.org.cn/MulanPSL2
 *
 * THIS SOFTWARE IS PROVIDED ON AN "AS IS" BASIS, WITHOUT WARRANTIES OF ANY KIND,
 * EITHER EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT,
 * MERCHANTABILITY OR FIT FOR A PARTICULAR PURPOSE.
 *
 * See the Mulan PSL v2 for more details.
 ***************************************************************************************/
 package xiangshan.mem

 import chisel3._
 import chisel3.util._
 import org.chipsalliance.cde.config._
 import xiangshan._
 import xiangshan.backend.rob.{RobPtr, RobLsqIO}
 import xiangshan.ExceptionNO._
 import xiangshan.cache._
 import utils._
 import utility._
 import xiangshan.backend.Bundles
 import xiangshan.backend.Bundles.{DynInst, MemExuOutput}
 import xiangshan.backend.fu.FuConfig.LduCfg

 class NCBufferEntry(entryIndex: Int)(implicit p: Parameters) extends XSModule
   with HasCircularQueuePtrHelper
   with HasLoadHelper
 {
   val io = IO(new Bundle() {
     val id = Input(UInt())

     val redirect = Flipped(Valid(new Redirect))

     // client requests
     val req = Flipped(Valid(new LqWriteBundle))

     // rerequest nc_with_data to loadunit
     val ncOut = DecoupledIO(new LsPipelineBundle)

     // uncache io
     val uncache = new UncacheWordIO

     // flush this entry
     val flush = Output(Bool())

     // exception generated by outer bus
     val exception = Valid(new LqWriteBundle)
   })

   val req_valid = RegInit(false.B)
   val req = Reg(new LqWriteBundle)

   val s_idle :: s_req :: s_resp :: s_wait :: Nil = Enum(4)
   val uncacheState = RegInit(s_idle)
   val uncacheData = Reg(io.uncache.resp.bits.data.cloneType)
   val nderr = RegInit(false.B)

   // enqueue
   when (req_valid && req.uop.robIdx.needFlush(io.redirect)) {
     req_valid := false.B
   } .elsewhen (io.req.valid) {
     XSError(req_valid, p"UncacheNCBuffer: You can not write an valid entry: $entryIndex")
     req_valid := true.B
     req := io.req.bits
     nderr := false.B
   } .elsewhen (io.ncOut.fire) {
     req_valid := false.B
   }

   io.flush := req_valid && req.uop.robIdx.needFlush(io.redirect)
   /**
     * NC operations
     *
     * States:
     * (1) s_idle: wait for nc req from loadunit
     * (2) s_req: wait to be sent to uncache channel until getting new nc req and uncache ready
     * (3) s_resp: wait for response from uncache channel
     * (4) s_wait: wait loadunit for A to receive nc_with_data req
     */

   switch (uncacheState) {
     is (s_idle) {
       when (req_valid) {
         uncacheState := s_req
       }
     }
     is (s_req) {
       when (io.uncache.req.fire) {
         uncacheState := s_resp
       }
     }
     is (s_resp) {
       when (io.uncache.resp.fire) {
         uncacheState := s_wait
       }
     }
     is (s_wait) {
       when (io.ncOut.fire) {
         uncacheState := s_idle // ready for next mmio
       }
     }
   }

   io.uncache.req.valid     := uncacheState === s_req
   io.uncache.req.bits      := DontCare
   io.uncache.req.bits.cmd  := MemoryOpConstants.M_XRD
   io.uncache.req.bits.data := DontCare
   io.uncache.req.bits.addr := req.paddr
   io.uncache.req.bits.mask := Mux(req.paddr(3), req.mask(15, 8), req.mask(7, 0))
   io.uncache.req.bits.id   := io.id
   io.uncache.req.bits.instrtype := DontCare
   io.uncache.req.bits.replayCarry := DontCare
   io.uncache.req.bits.atomic := false.B
   io.uncache.req.bits.nc := true.B

   io.uncache.resp.ready := true.B

   when (io.uncache.req.fire) {
     XSDebug("uncache req: pc %x addr %x data %x op %x mask %x\n",
       req.uop.pc,
       io.uncache.req.bits.addr,
       io.uncache.req.bits.data,
       io.uncache.req.bits.cmd,
       io.uncache.req.bits.mask
     )
   }

   // (3) response from uncache channel
   when (io.uncache.resp.fire) {
     uncacheData := io.uncache.resp.bits.data
     nderr := io.uncache.resp.bits.nderr
   }

   // uncache writeback
   val selUop = req.uop
   val func = selUop.fuOpType
   val raddr = req.paddr
   val rdataSel = LookupTree(raddr(2, 0), List(
       "b000".U -> uncacheData(63,  0),
       "b001".U -> uncacheData(63,  8),
       "b010".U -> uncacheData(63, 16),
       "b011".U -> uncacheData(63, 24),
       "b100".U -> uncacheData(63, 32),
       "b101".U -> uncacheData(63, 40),
       "b110".U -> uncacheData(63, 48),
       "b111".U -> uncacheData(63, 56)
     ))
   val rdataPartialLoad = rdataHelper(selUop, rdataSel)

   io.ncOut.valid := (uncacheState === s_wait)
   io.ncOut.bits := DontCare
   io.ncOut.bits.uop := selUop
   io.ncOut.bits.uop.lqIdx := req.uop.lqIdx
   io.ncOut.bits.uop.exceptionVec(loadAccessFault) := nderr
   io.ncOut.bits.data := rdataPartialLoad
   io.ncOut.bits.paddr := req.paddr
   io.ncOut.bits.vaddr := req.vaddr
   io.ncOut.bits.nc := true.B


   io.exception.valid := io.ncOut.fire
   io.exception.bits := req
   io.exception.bits.uop.exceptionVec(loadAccessFault) := nderr


   when (io.ncOut.fire) {
     req_valid := false.B

     XSInfo("int load miss write to cbd robidx %d lqidx %d pc 0x%x mmio %x\n",
       io.ncOut.bits.uop.robIdx.asUInt,
       io.ncOut.bits.uop.lqIdx.asUInt,
       io.ncOut.bits.uop.pc,
       true.B
     )
   }

   // end
 }

 class NCBuffer(implicit p: Parameters) extends XSModule with HasCircularQueuePtrHelper {
   val io = IO(new Bundle() {
     // control
     val redirect = Flipped(Valid(new Redirect))

     //from loadunit
     val req = Vec(LoadPipelineWidth, Flipped(Valid(new LqWriteBundle)))

     //to loadunit: return response of nc with data
     val ncOut = Vec(LoadPipelineWidth, Decoupled(new LsPipelineBundle))

     // uncache io
     val uncache = new UncacheWordIO

     // rollback from frontend when NCBuffer is full
     val rollback = Output(Valid(new Redirect))

     // exception generated by outer bus
     val exception = Valid(new LqWriteBundle)
   })

   val entries = Seq.tabulate(LoadNCBufferSize)(i => Module(new NCBufferEntry(i)))

   // freelist: store valid entries index.
   // +---+---+--------------+-----+-----+
   // | 0 | 1 |      ......  | n-2 | n-1 |
   // +---+---+--------------+-----+-----+
   val freeList = Module(new FreeList(
     size = LoadNCBufferSize,
     allocWidth = LoadPipelineWidth,
     freeWidth = 4,
     enablePreAlloc = true,
     moduleName = "NCBuffer freelist"
   ))
   freeList.io := DontCare

   // set enqueue default
   entries.foreach {
     case (e) =>
       e.io.req.valid := false.B
       e.io.req.bits := DontCare
   }

   // set uncache default
   io.uncache.req.valid := false.B
   io.uncache.req.bits := DontCare
   io.uncache.resp.ready := false.B

   entries.foreach {
     case (e) =>
       e.io.uncache.req.ready := false.B
       e.io.uncache.resp.valid := false.B
       e.io.uncache.resp.bits := DontCare
   }

   // set writeback default
   for (w <- 0 until LoadPipelineWidth) {
     io.ncOut(w).valid := false.B
     io.ncOut(w).bits := DontCare
   }

   // enqueue
   // s1:
   val s1_req = VecInit(io.req.map(_.bits))
   val s1_valid = VecInit(io.req.map(_.valid))

   // s2: enqueue
   val s2_req = (0 until LoadPipelineWidth).map(i => {
     RegEnable(s1_req(i), s1_valid(i))})
   val s2_valid = (0 until LoadPipelineWidth).map(i => {
     RegNext(s1_valid(i)) &&
     !s2_req(i).uop.robIdx.needFlush(RegNext(io.redirect)) &&
     !s2_req(i).uop.robIdx.needFlush(io.redirect)
   })
   val s2_has_exception = s2_req.map(x => ExceptionNO.selectByFu(x.uop.exceptionVec, LduCfg).asUInt.orR)
   val s2_need_replay = s2_req.map(_.rep_info.need_rep)

   val s2_enqueue = Wire(Vec(LoadPipelineWidth, Bool()))
   for (w <- 0 until LoadPipelineWidth) {
     s2_enqueue(w) := s2_valid(w) && !s2_has_exception(w) && !s2_need_replay(w) && s2_req(w).nc
   }

   //
   val enqValidVec = Wire(Vec(LoadPipelineWidth, Bool()))
   val enqIndexVec = Wire(Vec(LoadPipelineWidth, UInt()))

   for (w <- 0 until LoadPipelineWidth) {
     freeList.io.allocateReq(w) := true.B
   }

   // freeList real-allocate
   for (w <- 0 until LoadPipelineWidth) {
     freeList.io.doAllocate(w) := enqValidVec(w)
   }

   for (w <- 0 until LoadPipelineWidth) {
     enqValidVec(w) := s2_enqueue(w) && freeList.io.canAllocate(w)

     val offset = PopCount(s2_enqueue.take(w))
     enqIndexVec(w) := freeList.io.allocateSlot(offset)
   }

   // TODO lyq: It's best to choose in robIdx order
   val uncacheReqArb = Module(new RRArbiterInit(io.uncache.req.bits.cloneType, LoadNCBufferSize))
   val ncOutArb = Module(new RRArbiterInit(io.ncOut(0).bits.cloneType, LoadNCBufferSize))

   entries.zipWithIndex.foreach {
     case (e, i) =>
       e.io.redirect <> io.redirect
       e.io.id := i.U

       // enqueue
       for (w <- 0 until LoadPipelineWidth) {
         when (enqValidVec(w) && (i.U === enqIndexVec(w))) {
           e.io.req.valid := true.B
           e.io.req.bits := s2_req(w)
         }
       }

       // uncache logic
       uncacheReqArb.io.in(i).valid := e.io.uncache.req.valid
       uncacheReqArb.io.in(i).bits := e.io.uncache.req.bits
       e.io.uncache.req.ready := uncacheReqArb.io.in(i).ready
       ncOutArb.io.in(i).valid := e.io.ncOut.valid
       ncOutArb.io.in(i).bits := e.io.ncOut.bits
       e.io.ncOut.ready := ncOutArb.io.in(i).ready

       when (i.U === io.uncache.resp.bits.id) {
         e.io.uncache.resp <> io.uncache.resp
       }
   }

   // uncache Request
   AddPipelineReg(uncacheReqArb.io.out, io.uncache.req, false.B)

   // uncache Writeback
   AddPipelineReg(ncOutArb.io.out, io.ncOut(0), false.B)

   // uncache exception
   io.exception.valid := Cat(entries.map(_.io.exception.valid)).orR
   io.exception.bits := ParallelPriorityMux(entries.map(e =>
     (e.io.exception.valid, e.io.exception.bits)
   ))

   // UncacheBuffer deallocate
   val freeMaskVec = Wire(Vec(LoadNCBufferSize, Bool()))

   // init
   freeMaskVec.map(e => e := false.B)

   // dealloc logic
   entries.zipWithIndex.foreach {
     case (e, i) =>
       when (e.io.ncOut.fire || e.io.flush) {
         freeMaskVec(i) := true.B
       }
   }

   freeList.io.free := freeMaskVec.asUInt

   /**
     * Uncache rollback detection
     *
     * When uncache loads enqueue, it searches uncache loads, They can not enqueue and need re-execution.
     *
     * Cycle 0: uncache enqueue.
     * Cycle 1: Select oldest uncache loads.
     * Cycle 2: Redirect Fire.
     *   Choose the oldest load from LoadPipelineWidth oldest loads.
     *   Prepare redirect request according to the detected rejection.
     *   Fire redirect request (if valid)
     */
   //               Load_S3  .... Load_S3
   // stage 0:        lq            lq
   //                 |             | (can not enqueue)
   // stage 1:        lq            lq
   //                 |             |
   //                 ---------------
   //                        |
   // stage 2:               lq
   //                        |
   //                     rollback req
   def selectOldestRedirect(xs: Seq[Valid[Redirect]]): Vec[Bool] = {
     val compareVec = (0 until xs.length).map(i => (0 until i).map(j => isAfter(xs(j).bits.robIdx, xs(i).bits.robIdx)))
     val resultOnehot = VecInit((0 until xs.length).map(i => Cat((0 until xs.length).map(j =>
       (if (j < i) !xs(j).valid || compareVec(i)(j)
       else if (j == i) xs(i).valid
       else !xs(j).valid || !compareVec(j)(i))
     )).andR))
     resultOnehot
   }
   val reqNeedCheck = VecInit((0 until LoadPipelineWidth).map(w =>
     s2_enqueue(w) && !enqValidVec(w)
   ))
   val reqSelUops = VecInit(s2_req.map(_.uop))
   val allRedirect = (0 until LoadPipelineWidth).map(i => {
     val redirect = Wire(Valid(new Redirect))
     redirect.valid := reqNeedCheck(i)
     redirect.bits             := DontCare
     redirect.bits.isRVC       := reqSelUops(i).preDecodeInfo.isRVC
     redirect.bits.robIdx      := reqSelUops(i).robIdx
     redirect.bits.ftqIdx      := reqSelUops(i).ftqPtr
     redirect.bits.ftqOffset   := reqSelUops(i).ftqOffset
     redirect.bits.level       := RedirectLevel.flush
     redirect.bits.cfiUpdate.target := reqSelUops(i).pc // TODO: check if need pc
     redirect.bits.debug_runahead_checkpoint_id := reqSelUops(i).debugInfo.runahead_checkpoint_id
     redirect
   })
   val oldestOneHot = selectOldestRedirect(allRedirect)
   val oldestRedirect = Mux1H(oldestOneHot, allRedirect)
   val lastCycleRedirect = Wire(Valid(new Redirect))
   lastCycleRedirect.valid := RegNext(io.redirect.valid)
   lastCycleRedirect.bits := RegEnable(io.redirect.bits, io.redirect.valid)
   val lastLastCycleRedirect = Wire(Valid(new Redirect))
   lastLastCycleRedirect.valid := RegNext(lastCycleRedirect.valid)
   lastLastCycleRedirect.bits := RegEnable(lastCycleRedirect.bits, lastCycleRedirect.valid)
   io.rollback.valid := GatedValidRegNext(oldestRedirect.valid &&
                       !oldestRedirect.bits.robIdx.needFlush(io.redirect) &&
                       !oldestRedirect.bits.robIdx.needFlush(lastCycleRedirect) &&
                       !oldestRedirect.bits.robIdx.needFlush(lastLastCycleRedirect))
   io.rollback.bits := RegEnable(oldestRedirect.bits, oldestRedirect.valid)

   //  perf counter
   val validCount = freeList.io.validCount
   val allowEnqueue = !freeList.io.empty
   QueuePerf(LoadNCBufferSize, validCount, !allowEnqueue)

   XSPerfAccumulate("mmioCycle", VecInit(uncacheReqArb.io.in.map(_.fire)).asUInt.orR)
   XSPerfAccumulate("mmioCnt", io.uncache.req.fire)
   XSPerfAccumulate("mmio_writeback_success", io.ncOut(0).fire)
   XSPerfAccumulate("mmio_writeback_blocked", io.ncOut(0).valid && !io.ncOut(0).ready)
   XSPerfAccumulate("uncache_full_rollback", io.rollback.valid)

   val perfEvents: Seq[(String, UInt)] = Seq(
     ("mmioCycle", VecInit(uncacheReqArb.io.in.map(_.fire)).asUInt.orR),
     ("mmioCnt", io.uncache.req.fire),
     ("mmio_writeback_success", io.ncOut(0).fire),
     ("mmio_writeback_blocked", io.ncOut(0).valid && !io.ncOut(0).ready),
     ("uncache_full_rollback",  io.rollback.valid)
   )
   // end
 }
